{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38ae8a4a-6250-4861-8ae3-b08bb2d3adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "244a39bc-d646-49c9-b397-27dee36b77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "h = 8\n",
    "d_model = 512\n",
    "d_k = d_model // h\n",
    "seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52c0cd67-4982-4a37-9490-71534d7f6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of the self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k, d_model, seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wq = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        self.wk = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        self.wv = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B (batch_size), T (seq_length), C (d_model)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, d_k)\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        # print(k.shape)\n",
    "\n",
    "        # (batch_size, seq_length, d_k) * (batch_size, d_k, seq_length) \n",
    "        # -> (batch_size, seq_length, seq_length)\n",
    "        qkt = q @ k.transpose(-2,-1) / math.sqrt(d_k)\n",
    "        \n",
    "        # apply mask (seq_length*seq_length) now\n",
    "        mask = torch.tril((torch.ones(seq_length,seq_length)==1))==False\n",
    "        qkt = qkt.masked_fill(mask, -torch.inf)\n",
    "        \n",
    "        sm = F.softmax(qkt, dim=-1)\n",
    "\n",
    "        # (batch_size, seq_length, seq_length) * (batch_size, seq_length, d_model)\n",
    "        # -> (batch_size, seq_length, d_model)\n",
    "        att  = sm @ v\n",
    "        \n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac673964-0bda-4245-9205-a7d1d03cdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Heads of self-attention in parallel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, num_embed, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(3)\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(\n",
    "                    d_k=head_size,\n",
    "                    d_model=num_embed,\n",
    "                    seq_length=block_size,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(num_embed, num_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output of the self-attention\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # apply the linear projection layer\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba8eed14-0781-4a7f-bb86-73db70511598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my own implementation\n",
    "class MultiHeadAttention2(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.d_k = int(d_model // h)\n",
    "\n",
    "        torch.manual_seed(3)\n",
    "        self.mheads = nn.ModuleList([AttentionHead(self.d_k, d_model, seq_length) for i in range(h)])\n",
    "        self.wo = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        yh_cat = self.mheads[0](x)\n",
    "        for i in range(1, self.h):\n",
    "            yhi = self.mheads[i](x)\n",
    "            yh_cat = torch.cat((yh_cat, yhi),-1)\n",
    "\n",
    "        y = self.wo(yh_cat)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e39e7121-ec9c-4597-8b3f-b5612601d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha1 = MultiHeadAttention(h, d_model//h, d_model, seq_length)\n",
    "mha2 = MultiHeadAttention2(h, d_model, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb2db898-8ebc-4ac6-90db-b8f4f1a25c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac440bb9-4630-42f4-9024-a5811391d4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mha1(x)-mha2(x)).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb5887e5-23ed-489b-bd2d-bb91854eb5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb40870-d56b-4594-97ff-d6b221fde320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91eca8-15f7-4901-ae4d-65a30eae85cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
