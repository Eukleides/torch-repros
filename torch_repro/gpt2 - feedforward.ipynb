{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca47fa5-0ac1-4471-b5d9-6433a7fae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8fef2d-65b9-4e4f-a771-90a59ae51831",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "h = 8\n",
    "d_model = 512\n",
    "d_k = d_model // h\n",
    "seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b106fd-71d8-4abf-8e6c-80bb351defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of the self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k, d_model, seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wq = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        self.wk = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        self.wv = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B (batch_size), T (seq_length), C (d_model)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, d_k)\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        # print(k.shape)\n",
    "\n",
    "        # (batch_size, seq_length, d_k) * (batch_size, d_k, seq_length) \n",
    "        # -> (batch_size, seq_length, seq_length)\n",
    "        qkt = q @ k.transpose(-2,-1) / math.sqrt(d_k)\n",
    "        \n",
    "        # apply mask (seq_length*seq_length) now\n",
    "        mask = torch.tril((torch.ones(seq_length,seq_length)==1))==False\n",
    "        qkt = qkt.masked_fill(mask, -torch.inf)\n",
    "        \n",
    "        sm = F.softmax(qkt, dim=-1)\n",
    "\n",
    "        # (batch_size, seq_length, seq_length) * (batch_size, seq_length, d_model)\n",
    "        # -> (batch_size, seq_length, d_model)\n",
    "        att  = sm @ v\n",
    "        \n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e9cb88-e3b6-4171-8890-150ea0d40865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.d_k = int(d_model // h)\n",
    "\n",
    "        torch.manual_seed(3)\n",
    "        self.mheads = nn.ModuleList([AttentionHead(self.d_k, d_model, seq_length) for i in range(h)])\n",
    "        self.wo = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        yh_cat = self.mheads[0](x)\n",
    "        for i in range(1, self.h):\n",
    "            yhi = self.mheads[i](x)\n",
    "            yh_cat = torch.cat((yh_cat, yhi),-1)\n",
    "\n",
    "        y = self.wo(yh_cat)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505bff22-066c-4f83-a107-b8f71c9129a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear layer followed by ReLu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(3)\n",
    "        self.net = nn.Sequential(\n",
    "            # in the Attention is All You Need paper\n",
    "            # authors are using the size of the ffwd layer 2048\n",
    "            # and the output of the model is 512\n",
    "            # so we apply the same factor of 4\n",
    "            nn.Linear(num_embed, 4 * num_embed),\n",
    "            nn.GELU(),\n",
    "            # apply the linear projection layer\n",
    "            nn.Linear(4 * num_embed, num_embed),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9675ad5f-3dcf-4a44-96c6-a2d5fd949fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward2(nn.Module):\n",
    "    # my own implementation\n",
    "    def __init__(self, num_embed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(3)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "          nn.Linear(in_features=num_embed,out_features=4*num_embed,bias=True),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(in_features=4*num_embed,out_features=num_embed,bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.ff(x))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f5ed66-10e7-4996-8257-4e6be7f3733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee56f7e3-8733-452e-bd3d-f27029dec437",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(d_model)\n",
    "ff2 = FeedForward2(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a43a0c0-edb3-409f-bc84-a1477650d5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 512]), torch.Size([3, 5, 512]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff(x).shape, ff2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1860bb57-806c-4da7-84b8-3a7a164906ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ff(x)-ff2(x)).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f3d38-e314-4a31-b0e0-71e56f301e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d63be5-b4e2-4134-b2cf-5ddb617482da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
