{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca47fa5-0ac1-4471-b5d9-6433a7fae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8fef2d-65b9-4e4f-a771-90a59ae51831",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "h = 8\n",
    "d_model = 512\n",
    "d_k = d_model // h\n",
    "seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b106fd-71d8-4abf-8e6c-80bb351defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, d_k, d_model, seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wq = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        self.wk = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        self.wv = nn.Linear(in_features=d_model,out_features=d_k,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B (batch_size), T (seq_length), C (d_model)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, d_k)\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        # print(k.shape)\n",
    "\n",
    "        # (batch_size, seq_length, d_k) * (batch_size, d_k, seq_length) \n",
    "        # -> (batch_size, seq_length, seq_length)\n",
    "        qkt = q @ k.transpose(-2,-1) / math.sqrt(d_k)\n",
    "        \n",
    "        # apply mask (seq_length*seq_length) now\n",
    "        mask = torch.tril((torch.ones(seq_length,seq_length)==1))==False\n",
    "        qkt = qkt.masked_fill(mask, -torch.inf)\n",
    "        \n",
    "        sm = F.softmax(qkt, dim=-1)\n",
    "\n",
    "        # (batch_size, seq_length, seq_length) * (batch_size, seq_length, d_model)\n",
    "        # -> (batch_size, seq_length, d_model)\n",
    "        att  = sm @ v\n",
    "        \n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e9cb88-e3b6-4171-8890-150ea0d40865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.d_k = int(d_model // h)\n",
    "\n",
    "        self.mheads = nn.ModuleList([AttentionHead(self.d_k, d_model, seq_length) for i in range(h)])\n",
    "        self.wo = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        yh_cat = self.mheads[0](x)\n",
    "        for i in range(1, self.h):\n",
    "            yhi = self.mheads[i](x)\n",
    "            yh_cat = torch.cat((yh_cat, yhi),-1)\n",
    "\n",
    "        y = self.wo(yh_cat)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9675ad5f-3dcf-4a44-96c6-a2d5fd949fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "          nn.Linear(in_features=d_model,out_features=4*d_model,bias=True),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(in_features=4*d_model,out_features=d_model,bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.ff(x))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182c2a3a-ca54-4be6-8739-a4d5454a2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This calss will group together MultiHead Attention and\n",
    "    FeedForward NN, so that we can copy it in Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(3)\n",
    "        \n",
    "        self.sa = MultiHeadAttention(\n",
    "            h=h,\n",
    "            d_model=d_model,\n",
    "            seq_length=seq_length,\n",
    "        )\n",
    "        self.ffwd = FeedForward(d_model=d_model)\n",
    "        # add the layer normalization\n",
    "        self.ln1 = nn.LayerNorm([d_model])\n",
    "        self.ln2 = nn.LayerNorm([d_model])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"x +\" is the skip (or residual) connection\n",
    "        # it helps with optimization\n",
    "        # also we apply layer normalization before self-attention\n",
    "        # and feed-forward (a reshufle from original paper)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d889e771-e3a3-4c14-922e-753b9cfbc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock2(nn.Module):\n",
    "    # my own implementation\n",
    "    \n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(3)\n",
    "        self.mha = MultiHeadAttention(h, d_model, seq_length)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.ln1 = nn.LayerNorm([d_model]);\n",
    "        self.ln2 = nn.LayerNorm([d_model]);\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f5ed66-10e7-4996-8257-4e6be7f3733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee56f7e3-8733-452e-bd3d-f27029dec437",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TransformerBlock(h, d_model, seq_length)\n",
    "tb2 = TransformerBlock2(h, d_model, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a43a0c0-edb3-409f-bc84-a1477650d5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tb(x)-tb2(x)).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f3d38-e314-4a31-b0e0-71e56f301e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
