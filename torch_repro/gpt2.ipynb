{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca47fa5-0ac1-4471-b5d9-6433a7fae59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b106fd-71d8-4abf-8e6c-80bb351defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, d_k, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_k = d_k\n",
    "        self.seq_length = seq_length\n",
    "        self.wq = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.wk = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.wv = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B (batch_size), T (seq_length), C (d_model)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, d_k)\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        # (batch_size, seq_length, d_k) * (batch_size, d_k, seq_length) \n",
    "        # -> (batch_size, seq_length, seq_length)\n",
    "        qkt = q @ k.transpose(-2,-1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # apply mask (seq_length*seq_length) now\n",
    "        mask = torch.tril((torch.ones(self.seq_length, self.seq_length)==1))==False\n",
    "        qkt = qkt.masked_fill(mask, -torch.inf)\n",
    "        \n",
    "        sm = F.softmax(qkt, dim=-1)\n",
    "\n",
    "        # (batch_size, seq_length, seq_length) * (batch_size, seq_length, d_model)\n",
    "        # -> (batch_size, seq_length, d_model)\n",
    "        att  = sm @ v\n",
    "        \n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e9cb88-e3b6-4171-8890-150ea0d40865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.d_k = int(d_model // h)\n",
    "        \n",
    "        self.mheads = nn.ModuleList([AttentionHead(self.d_k, d_model, seq_length) for i in range(h)])\n",
    "        self.wo = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        yh_cat = self.mheads[0](x)\n",
    "        for i in range(1, self.h):\n",
    "            yhi = self.mheads[i](x)\n",
    "            yh_cat = torch.cat((yh_cat, yhi),-1)\n",
    "\n",
    "        y = self.wo(yh_cat)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9675ad5f-3dcf-4a44-96c6-a2d5fd949fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "          nn.Linear(in_features=d_model,out_features=4*d_model),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(in_features=4*d_model,out_features=d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.ff(x))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d889e771-e3a3-4c14-922e-753b9cfbc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):    \n",
    "    def __init__(self, h, d_model, seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(h, d_model, seq_length)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.ln1 = nn.LayerNorm([d_model]);\n",
    "        self.ln2 = nn.LayerNorm([d_model]);\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79a5829-c257-49b3-92ca-2576c02dc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        if model_name.upper()=='GPT2':\n",
    "            self.N = 12 # number of transformer blocks\n",
    "            self.h = 12 # number of heads\n",
    "            self.d_model = 768 # token embedding size\n",
    "            self.seq_length = 1024 # context size\n",
    "            self.vocab_size = 50257 # number of tokens in vocabulary\n",
    "        else:\n",
    "            throw(f'Unrecognised model {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f159b5fc-c172-4b05-a4ba-039a93b2ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):   \n",
    "    def __init__(self, config=Config()):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(3)\n",
    "        self.cfg = config\n",
    "        self.inp_emb = nn.Embedding(self.cfg.vocab_size, self.cfg.d_model)\n",
    "        # each (integer) position in the seq_length tokens will get its own embedding\n",
    "        self.pos_emb = nn.Embedding(self.cfg.seq_length, self.cfg.d_model)\n",
    "\n",
    "        self.blist = nn.ModuleList([TransformerBlock(\n",
    "                                self.cfg.h, \n",
    "                                self.cfg.d_model, \n",
    "                                self.cfg.seq_length) \n",
    "                              for _ in range(self.cfg.N)])\n",
    "        self.tblocks = nn.Sequential(*self.blist)\n",
    "\n",
    "        self.ln_out = nn.LayerNorm([self.cfg.d_model]);\n",
    "        self.lin_out = nn.Linear(in_features=self.cfg.d_model,\n",
    "                                 out_features=self.cfg.vocab_size,\n",
    "                                 bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length) -> (batch_size, seq_length, d_model)\n",
    "        x = self.inp_emb(x)\n",
    "        # (batch_size, seq_length, d_model) + (seq_length, d_model) ->\n",
    "        # (batch_size, seq_length, d_model)\n",
    "        x = x + self.pos_emb(torch.arange(self.cfg.seq_length))\n",
    "        # go through the N transformer blocks\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, d_model)\n",
    "        x = self.tblocks(x)\n",
    "        # normalise\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, d_model) \n",
    "        x = self.ln_out(x)\n",
    "        # (batch_size, seq_length, d_model) -> (batch_size, seq_length, vocab_size)\n",
    "        x = self.lin_out(x)\n",
    "        \n",
    "        return x       \n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # x is (batch_size, seq_length) array of indices in the current context\n",
    "            # (batch_size, seq_length) -> (batch_size, seq_length, vocab_size)\n",
    "            y = self(x)\n",
    "            # get the last element of (seq_length) tokens\n",
    "            y_last = y[:,-1:,:]\n",
    "            # use softmax to convert into probs\n",
    "            sm = F.softmax(y_last, dim=-1)\n",
    "            # sample from a vector of integers [0,vocab_size] using that probability distribution\n",
    "            # y_next -> (batch_size, 1)\n",
    "            x_next = torch.multinomial(sm.view(-1, self.cfg.vocab_size), num_samples=1, replacement=True)\n",
    "            # append a one-hot vector of vocab_size to the end of x\n",
    "            # output share to be (batch_size, seq_length)\n",
    "            x = torch.cat((x,x_next),-1)[:,1:]\n",
    "\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8fef2d-65b9-4e4f-a771-90a59ae51831",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb4569f-838a-4851-8e78-19bf7502d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "# self.N = 12 # number of transformer blocks\n",
    "# self.h = 12 # number of heads\n",
    "# self.d_model = 768 # token embedding size\n",
    "# self.seq_length = 1024 # context size\n",
    "# self.vocab_size = 50257 # number of tokens in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f5ed66-10e7-4996-8257-4e6be7f3733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 50257,(batch_size, cfg.seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee56f7e3-8733-452e-bd3d-f27029dec437",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a7f3d38-e314-4a31-b0e0-71e56f301e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = t.generate(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77585390-20ea-43d9-8209-c165be07c53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1024]), torch.Size([10, 1024]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077a0f2-66eb-48c0-a6c4-e4086dfc7193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
